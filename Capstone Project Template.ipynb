{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "##### As part of this project we will model the immigration and weather data of United States to understand the impact of temperature on the travel pattern of passengers coming to the United states over the past years for the past 50 years.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope: \n",
    "----------------------\n",
    "##### As part of this project we will model the immigration and weather data of the United States to understand the impact of temperature on the travel pattern of passengers coming to the United States over the past years for the past 50 years i.e since 1970s. More specifically we want to answer the questions below:\n",
    "1. Temperature influence over the years on the choice of destination city in the U.S.\n",
    "2. Temperature influence over the years on the choice of arrival date of travellers aggregated across the various date dimensions i.e. year, month, quarter etc.\n",
    "\n",
    "#### Source Datasets:\n",
    "-------------------------------\n",
    "##### Below are the datasets that we will used across the project.\n",
    "1. i94_apr16_sub.sas7bdat: This immigration data comes from the US National Tourism and Trade Office and contains information present on the I94 form which is the arrival/departure report card for the international visitors.\n",
    "2. GlobalLandTemperaturesByCity.csv: This dataset contains the world temperature recorded over various years for various cities across countries. We would only be looking at the temperature data of United States since 1970s\n",
    "3. I94PORT_lookup.csv: This file is generated based on the dictionary provided for i94port column in the I94_SAS_Labels_Descriptions.SAS file. Since we are only interested in US port of entry city and states, non US port of entries such as Canada, Mexico , Belgium etc as well as invalid, collapsed, no port code data have been dropped while creating the file.\n",
    "4. I94CIT_I94RES_lookup.csv: This file is generated based on dictionary provided for i94cit and i94res column in the I94_SAS_Labels_Descriptions.SAS file.\n",
    "\n",
    "#### Target tables:\n",
    "----------------------\n",
    "##### Below are the target table details:\n",
    "1. Fact Table : fact_immigration\n",
    "2. Dimension Tables: dim_temperature, dim_date\n",
    "3. Format: parquet\n",
    "\n",
    "#### ETL Pipeline Design:\n",
    "-------------------------\n",
    "##### The ETL pipeline will leverage Spark's in memory processing capabilities which is very fast and will loads the data as follows:\n",
    "1. Create a SparkSession and read the i94_apr16_sub.sas7bdat, GlobalLandTemperaturesByCity.csv, I94PORT_lookup.csv and I94CIT_I94RES_lookup.csv input files to create spark dataframes.\n",
    "2. Select the desired columns for each of the target tables and perform the required transformations.\n",
    "3. Save the transformed dataframe with the required partitioning in parquet format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 immigration data which is in sas format and load it in a pandas dataframe.\n",
    "fname_i94 = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(fname_i94, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore the first 5 rows of the i94 travel data.\n",
    "df_i94.head()\n",
    "# Explore the columns and the data types for i94 travel data.\n",
    "df_i94.info()\n",
    "# Explore the value counts for i94 travel data.\n",
    "df_i94.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the global temperature data which is in csv format and load it in a pandas dataframe.\n",
    "fname_global_temp = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_global_temp = pd.read_csv(fname_global_temp,parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore temperature data to check the columns and the corresponding value counts.\n",
    "df_global_temp.info()\n",
    "# Explore the first 5 rows of the global temperature data.\n",
    "df_global_temp.head()\n",
    "# Explore the value counts for global temperature data.\n",
    "df_global_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Since we are only interested in exploring U.S. data post 1970 ,apply a filter where \n",
    "# Country = 'United States' and dt >= 1st Jan 1970.\n",
    "df_us_temp = df_global_temp[df_global_temp['Country'] == 'United States']\n",
    "df_us_temp = df_us_temp[df_us_temp['dt'] >= '1970-01-01']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore us temperature data to check the columns and the corresponding value counts.\n",
    "df_us_temp.info()\n",
    "# Explore the first 5 rows of the us temperature data.\n",
    "df_us_temp.head()\n",
    "# Explore the value counts for us temperature data.\n",
    "df_us_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the I94 port data which is in csv format and load it in a pandas dataframe.\n",
    "fname_I94port = 'I94PORT_lookup.csv'\n",
    "df_I94port = pd.read_csv(fname_I94port,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore the value counts for the I94 port data alongwith the first 5 rows\n",
    "df_I94port.count()\n",
    "df_I94port.info()\n",
    "df_I94port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the I94 country data which is in csv format and load it in a pandas dataframe.\n",
    "fname_I94cit_res = 'I94CIT_I94RES_lookup.csv'\n",
    "df_I94cit_res = pd.read_csv(fname_I94cit_res,quotechar=\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore the value counts for the I94 country data alongwith the first 5 rows\n",
    "df_I94cit_res.count()\n",
    "df_I94cit_res.info()\n",
    "df_I94cit_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Instantiate py spark session and read in the i94 immigration data in spark dataframes\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark_i94 =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write the i94 immigration data to parquet folder locally and read from it.\n",
    "df_spark_i94.write.parquet(\"sas_data\")\n",
    "df_spark_i94=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the us temp pandas dataframe to spark dataframe \n",
    "df_spark_us_temp = spark.createDataFrame(df_us_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the i94 port pandas dataframe to spark dataframe \n",
    "mySchema_i94port = StructType([ StructField(\"code\", StringType(), True)\\\n",
    "                       ,StructField(\"city\", StringType(), True)\\\n",
    "                       ,StructField(\"state\", StringType(), True)])\n",
    "\n",
    "df_spark_i94port = spark.createDataFrame(df_I94port,schema=mySchema_i94port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the country pandas dataframe to spark dataframe \n",
    "mySchema_i94cit_res = StructType([ StructField(\"code\", LongType(), True)\\\n",
    "                       ,StructField(\"country\", StringType(), True)])\n",
    "df_spark_i94cit_res = spark.createDataFrame(df_I94cit_res,schema=mySchema_i94cit_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the schema of the i94  immigration data alongwith the value counts\n",
    "df_spark_i94.printSchema()\n",
    "print(df_spark_i94.count())\n",
    "df_spark_i94.show(5,truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Lets only select the columns below from the immigration dataset which will be part of the final data model and which we will explore futher.\n",
    "1. i94cit\n",
    "2. i94res\n",
    "3. i94port\n",
    "4. arrdate\n",
    "5. depdate\n",
    "6. i94mode\n",
    "7. i94visa\n",
    "8. gender \n",
    "9. biryear\n",
    "10. cicid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select a subset of columns from the immigration tbl and tally the record count.\n",
    "df_spark_i94 = df_spark_i94.select(\"cicid\",\"i94cit\",\"i94res\",\"i94port\",\"arrdate\",\"depdate\",\"i94mode\",\n",
    "                   \"i94visa\",\"gender\",\"i94bir\",\"biryear\").distinct()\n",
    "\n",
    "df_spark_i94.select(\"cicid\",\"i94cit\",\"i94res\",\"i94port\",\"arrdate\",\"depdate\",\"i94mode\",\n",
    "                   \"i94visa\",\"gender\",\"i94bir\",\"biryear\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the schema of the us temperature data alongwith the value counts\n",
    "df_spark_us_temp.printSchema()\n",
    "print(df_spark_us_temp.count())\n",
    "df_spark_us_temp.show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the schema of the i94 port data alongwith the value counts\n",
    "df_spark_i94port.printSchema()\n",
    "print(df_spark_i94port.count())\n",
    "df_spark_i94port.show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the schema of the i94 country data alongwith the value counts\n",
    "df_spark_i94cit_res.printSchema()\n",
    "print(df_spark_i94cit_res.count())\n",
    "df_spark_i94cit_res.show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Register the spark dataframes below as temporary table.\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')\n",
    "df_spark_i94cit_res.createOrReplaceTempView('country_tbl')\n",
    "df_spark_i94port.createOrReplaceTempView('port_of_entry_tbl')\n",
    "df_spark_us_temp.createOrReplaceTempView('us_temp_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets the check cicid column to see the distinct values:\n",
    "spark.sql(\"\"\" select distinct count(cicid)\n",
    "              from immigration_tbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Lets translate the code values in each of the fields below in the i94 immigration data to add suitable description based on the data dictionary provided in the workspace\n",
    "1. i94cit,i94res ==> Will be used to add country of origin and residence \n",
    "2. i94mode ==> Will be used to add the actual mode of transport\n",
    "3. i94visa ==> Will be used to add the visa type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if the country of origin codes in the immigration dataset is invalid and get the counts\n",
    "spark.sql(\"\"\"select distinct i94cit\n",
    "             from immigration_tbl where\n",
    "             i94cit not in (\n",
    "             select distinct code from country_tbl)\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl where\n",
    "             i94cit not in (\n",
    "             select distinct code from country_tbl)\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if the country of residence codes in the immigration dataset is invalid and get the counts\n",
    "spark.sql(\"\"\"select distinct i94res\n",
    "             from immigration_tbl where\n",
    "             i94res not in (\n",
    "             select distinct code from country_tbl)\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl where\n",
    "             i94res not in (\n",
    "             select distinct code from country_tbl)\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Looks like we have about 386189 rows where the country of origin is an invalid number. But since all the rows have a valid country of residence codes, we will not drop such records. \n",
    "##### We will do an inner join while determing the country of residence and a left outer join while determining the country of origin i.e country of residence is mandatory but not country of origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the country of residence and country of origin columns in the immigration dataset.\n",
    "df_spark_i94 = spark.sql(\"\"\"select imm.*, cntry.country as i94_country_origin\n",
    "             from immigration_tbl imm\n",
    "             left outer join country_tbl cntry\n",
    "             on imm.i94cit = cntry.code\"\"\")\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')\n",
    "\n",
    "df_spark_i94 = spark.sql(\"\"\"select imm.*, cntry.country as i94_country_residence\n",
    "             from immigration_tbl imm\n",
    "             inner join country_tbl cntry\n",
    "             on imm.i94res = cntry.code\"\"\")\n",
    "\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the country of residence and origin columns \n",
    "# alongwith the row count to check no records were dropped from the i94 immigration datasets\n",
    "spark.sql(\"select * from immigration_tbl\").limit(5).show()           \n",
    "spark.sql(\"select count(*) from immigration_tbl\").show()           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the the count of records from the immigration table where port of entry is some sample \n",
    "# Non US city or invalid port of entry\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl \n",
    "             where trim(i94port) in ('AUH','888','CLG')\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Since we are only interested in visitors travelling to U.S. with valid U.S. port of entry , we will do an inner join to eliminate non U.S. cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the port of entry columns in the immigration dataset.\n",
    "df_spark_i94 = spark.sql(\"\"\"select imm.*\n",
    "             from immigration_tbl imm\n",
    "             inner join port_of_entry_tbl prt\n",
    "             on trim(imm.i94port) = trim(prt.code)\"\"\")\n",
    "\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the port of entry alongwith the row count from the i94 immigration dataset. \n",
    "# Check the record count for the sample non US city or invalid port of entry is 0.\n",
    "spark.sql(\"select * from immigration_tbl\").limit(5).show()           \n",
    "spark.sql(\"select count(*) from immigration_tbl\").show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl \n",
    "             where trim(i94port) in ('AUH','888','CLG')\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the distinct values mode of travel in the immigration dataset \n",
    "spark.sql(\"\"\"select distinct i94mode,count(*)\n",
    "             from immigration_tbl\n",
    "             group by i94mode\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### We will only keep records where the mode of travel is by land, air and sea. We will drop the records where i94mode is either null or 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the mode of travel columns in the immigration dataset for air , sea and land.\n",
    "df_spark_i94 = spark.sql(\"\"\"select *,case \n",
    "                            when i94mode = 1.0 then 'air'\n",
    "                            when i94mode = 2.0 then 'sea'\n",
    "                            when i94mode = 3.0 then 'land'\n",
    "                            else 'Nan' end as i94_mode_travel\n",
    "                            from immigration_tbl\n",
    "                            where i94mode in (1.0,2.0,3.0)\"\"\")\n",
    "\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the mode of travel record alongwith the row count to check only 7422\n",
    "# records were dropped from a total of 2973390 records from the i94 immigration dataset.\n",
    "spark.sql(\"\"\"select *\n",
    "             from immigration_tbl\"\"\").limit(5).show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the distinct values visa type in the immigration dataset \n",
    "spark.sql(\"\"\"select distinct i94visa,count(*)\n",
    "             from immigration_tbl\n",
    "             group by i94visa\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the visa type description column in the immigration dataset for business , pleasure and student.\n",
    "df_spark_i94 = spark.sql(\"\"\"select *,case \n",
    "                            when i94visa = 1.0 then 'business'\n",
    "                            when i94visa = 2.0 then 'pleasure'\n",
    "                            when i94visa = 3.0 then 'student'\n",
    "                            else 'Nan' end as i94_visa_type\n",
    "                            from immigration_tbl\n",
    "                            where i94visa in (1.0,2.0,3.0)\"\"\")\n",
    "\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the visa type in the record alongwith the row count to check none of the  \n",
    "# records were dropped from the i94 immigration dataset. Number of records = 2965968.\n",
    "spark.sql(\"\"\"select *\n",
    "             from immigration_tbl\"\"\").limit(5).show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Lets translate the fields below in the i94 immigration data:\n",
    "arrdate,depdate: Since the SAS numeric date represents the number of days since 1st Jan 1960, we will need to convert these numeric dates into yyyy-mm-dd format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert arrival and departure date from SAS numeric format to yyyy-mm-dd date format\n",
    "df_spark_i94 = spark.sql(\"\"\"select *,\n",
    "                   date_add('1960-01-01', arrdate) as i94_arrival_date,\n",
    "                   date_add('1960-01-01',depdate) as i94_dep_date\n",
    "                   from immigration_tbl\"\"\")\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')\n",
    "spark.sql(\"select * from immigration_tbl\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets take a look at the min and max of arrival date\n",
    "spark.sql(\"\"\"select min(i94_arrival_date),max(i94_arrival_date)\n",
    "                    from immigration_tbl\"\"\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets take a look and see if we have any rows where both the arrival and departure dates are null\n",
    "spark.sql(\"\"\"select *\n",
    "             from immigration_tbl\n",
    "             where i94_arrival_date is null\n",
    "             and i94_dep_date is null\"\"\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets take a look and see if we have any rows where the arrival date is null but the departure dates \n",
    "# is not null\n",
    "spark.sql(\"\"\"select *\n",
    "             from immigration_tbl\n",
    "             where i94_arrival_date is null\n",
    "             and i94_dep_date is not null\"\"\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the gender columns\n",
    "spark.sql(\"\"\"select gender, count(*)\n",
    "             from immigration_tbl\n",
    "             group by gender\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### There are records with null gender values which seem to missing information. We will allow these records to be part of the final data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the i94bir and biryear column to see if there are any invalid records and the range of \n",
    "# values present in the field\n",
    "spark.sql(\"\"\"select max(i94bir), min(i94bir)\n",
    "             from immigration_tbl\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl\n",
    "             where i94bir is null\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select max(biryear), min(biryear)\n",
    "             from immigration_tbl\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl\n",
    "             where biryear is null\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Looks like i94bir contains negative values for age which is incorrect. We will use the the biryear column to calculate the age where the i94bir is < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the age column in the immigration dataset based on i94bir and biryear fields.\n",
    "df_spark_i94 = spark.sql(\"\"\"select *,case \n",
    "                            when i94bir < 0.0 then (double(year(current_date))-biryear)\n",
    "                            else i94bir end as i94_age\n",
    "                            from immigration_tbl\"\"\")\n",
    "\n",
    "df_spark_i94.createOrReplaceTempView('immigration_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the age field in the record alongwith the row count to check none of the  \n",
    "# records were dropped from the i94 immigration dataset. Number of records = 2965968\n",
    "spark.sql(\"\"\"select *\n",
    "             from immigration_tbl\"\"\").limit(5).show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from immigration_tbl\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Lets take a look at the temperature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the min and max dates when the temperature was recorded\n",
    "spark.sql(\"\"\"select min(dt),max(dt)\n",
    "             from us_temp_tbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if there are any rows where the temperature was recorded as NaN.\n",
    "spark.sql(\"\"\"select *\n",
    "             from us_temp_tbl\n",
    "             where AverageTemperature = 'NaN'\n",
    "             or AverageTemperatureUncertainty = 'NaN'  \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Lets cleanup the records where both the Average temperature column is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop the records where the AverageTemperature column is NaN.\n",
    "df_spark_us_temp = spark.sql(\"\"\"select *\n",
    "             from us_temp_tbl\n",
    "             where NOT(AverageTemperature = 'NaN')\"\"\")\n",
    "df_spark_us_temp.createOrReplaceTempView(\"us_temp_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the individual records and the total number of records in the us_temp_table\n",
    "spark.sql(\"\"\"select *\n",
    "             from us_temp_tbl\"\"\").limit(5).show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from us_temp_tbl\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if the rest of the columns contain any null values.\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from us_temp_tbl\n",
    "             where Latitude = 'NaN'\n",
    "             or Longitude = 'NaN'\n",
    "             or dt = 'NaN'\n",
    "             or city = 'NaN'\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from us_temp_tbl\n",
    "             where Latitude is null\n",
    "             or Longitude is null\n",
    "             or dt is null\n",
    "             or city is null\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if there are multiple rows for a given combination of dt,city,lattitude,longitude. \n",
    "# The record count should be equal to the total record count in us_temp_table.\n",
    "spark.sql(\"\"\"select distinct dt,city,Latitude,Longitude\n",
    "             from us_temp_tbl\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the port of entry code, state columns in the tempertaure dataset.\n",
    "df_spark_us_temp = spark.sql(\"\"\"select temp.*, trim(prt.code) as i94_port_code, trim(prt.state) as state\n",
    "             from us_temp_tbl temp\n",
    "             inner join port_of_entry_tbl prt\n",
    "             on trim(lower(temp.city)) = trim(lower(prt.city))\"\"\")\n",
    "\n",
    "df_spark_us_temp.createOrReplaceTempView('us_temp_tbl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validate the individual records and the total number of records in the us_temp_table\n",
    "spark.sql(\"\"\"select *\n",
    "             from us_temp_tbl\"\"\").limit(5).show()\n",
    "spark.sql(\"\"\"select count(*)\n",
    "             from us_temp_tbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "##### fact_immigration: The table  will contain the immigration details for travellers travelling in US.\n",
    "1. i94_cicid\n",
    "2. i94_country_origin\n",
    "3. i94_country_residence\n",
    "4. i94_port_code\n",
    "5. i94_arrival_date\n",
    "6. i94_dep_date\n",
    "7. i94_mode_travel\n",
    "8. i94_visa_type\n",
    "9. i94_gender \n",
    "10. i94_age\n",
    "11. year\n",
    "\n",
    "##### dim_temperature: The table will contain the temperature for USA across various cities post 1970.\n",
    "1. i94_port_code\n",
    "2. city \n",
    "3. state \n",
    "4. date\n",
    "5. avg_temperature \n",
    "6. avg_temperature_un \n",
    "7. latitude \n",
    "8. longitude\n",
    "9. year\n",
    "\n",
    "##### dim_date : The table will contain various date dimensions for the arrival dates from the immigration dataset.\n",
    "1. date\n",
    "2. year\n",
    "3. month\n",
    "4. day\n",
    "5. week\n",
    "6. weekday\n",
    "7. quarter\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "##### fact_immigration:\n",
    "1. Read the following datasets and convert it into spark dataframe: i94_apr16_sub.sas7bdat, I94PORT_lookup.csv, I94CIT_I94RES_lookup.csv\n",
    "2. Get the actual country names for the country of origin and country of residence codes values by joining i94_apr16_sub.sas7bdat and I94CIT_I94RES_lookup.csv dataframes.\n",
    "3. Filter out the invalid port of entries by joining i94_apr16_sub.sas7bdat and I94PORT_lookup.csv dataframes. \n",
    "4. Replace the mode of the travel, visa type codes with the actual values.Filter out the mode of travel values where mode of travel = land, air, sea.\n",
    "5. Get the age of the travelling using the age and the birth year information.\n",
    "6. Get the gender and cicid of the traveller.\n",
    "7. Convert the arrival date and the departure date from SAS numeric format to yyyy-mm-dd format and filter out the invalid records.\n",
    "8. Select the required columns to create the fact table and write it in parquet format partitioned by year of arrival date and i94 port code.\n",
    "\n",
    "##### dim_temperature:\n",
    "1. Read the GlobalLandTemperaturesByCity.csv file and convert to spark dataframe\n",
    "2. Check the temperature column for null values and filter out records where the average temperature columns value is null.\n",
    "3. Check the other required column for null values\n",
    "4. Join GlobalLandTemperaturesByCity.csv with the I94PORT_lookup.csv dataframes to get the i94 port of entry code and state  based on the city column.\n",
    "5. Select the required columns to create the temperature dimension table and write in parquet format partitioned by year of the date when the temperature was recorded.\n",
    "\n",
    "##### dim_date\n",
    "1. Get the arrival date from immigration fact table and extract various date dimensions such as year, month, day, week etc.\n",
    "2. Create the date dimension table and write it in parquet format partitioned by year of arrival date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the required columns to create the fact_immigration table.\n",
    "df_spark_i94 = spark.sql(\"\"\"select cicid as i94_cicid,\n",
    "             i94_country_origin,\n",
    "             i94_country_residence,\n",
    "             i94port as i94_port_code,\n",
    "             i94_arrival_date,\n",
    "             i94_dep_date,\n",
    "             i94_mode_travel,\n",
    "             i94_visa_type,\n",
    "             gender as i94_gender,\n",
    "             i94_age,\n",
    "             year(i94_arrival_date) as year\n",
    "             from immigration_tbl\"\"\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the required columns to create the dim_temperature table.\n",
    "df_spark_us_temp = spark.sql(\"\"\"select i94_port_code,\n",
    "             city,\n",
    "             state,\n",
    "             dt as date,\n",
    "             AverageTemperature as avg_temperature,\n",
    "             AverageTemperatureUncertainty as avg_temperature_un,\n",
    "             Latitude as latitude,\n",
    "             Longitude as longitude,\n",
    "             year(dt) as year\n",
    "             from us_temp_tbl \"\"\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the required columns to create the dim_date table.\n",
    "df_spark_date = df_spark_i94.selectExpr(\"i94_arrival_date as date\").distinct()\n",
    "df_spark_date = df_spark_date.withColumn(\"year\",F.year(\"date\"))\n",
    "df_spark_date = df_spark_date.withColumn(\"month\",F.month(\"date\"))\n",
    "df_spark_date = df_spark_date.withColumn(\"day\",F.dayofmonth(\"date\"))\n",
    "df_spark_date = df_spark_date.withColumn(\"week\",F.weekofyear(\"date\"))\n",
    "df_spark_date = df_spark_date.withColumn(\"weekday\",F.dayofweek(\"date\"))\n",
    "df_spark_date = df_spark_date.withColumn(\"quarter\",F.quarter(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write fact_immigration table to parquet files partitioned by year, i94_port_code \n",
    "df_spark_i94.write.mode(\"append\").partitionBy(\"year\",\"i94_port_code\").parquet(\"output/fact_immigration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dim_temperature table to parquet files partitioned by year.\n",
    "df_spark_us_temp.write.mode(\"append\").partitionBy(\"year\").parquet(\"output/dim_temperature/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dim_date table to parquet files partitioned by year.\n",
    "df_spark_date.write.mode(\"append\").partitionBy(\"year\").parquet(\"output/dim_date/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "* Check the total record counts for each of the 3 target tables\n",
    "* Check the sample records for each of the 3 target tables\n",
    "* Check if the mandatory columns in each of the 3 tables do not contain any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The record count for fact immigration table should be 2965968\n",
    "print(df_spark_i94.count())\n",
    "# Check the sample record\n",
    "df_spark_i94.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The record count for dimension temperature table should be 51449\n",
    "print(df_spark_us_temp.count())\n",
    "# Check the sample record\n",
    "df_spark_us_temp.show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The record count for dimension date table should be equal to the number of distinct arrival \n",
    "# dates in fact immigration table.\n",
    "print(df_spark_i94.select(\"i94_arrival_date\").distinct().count())\n",
    "print(df_spark_date.count())\n",
    "# Check the sample record\n",
    "df_spark_date.show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if the mandatory columns in each of the 3 tables do not contain any null values\n",
    "\n",
    "dq = {df_spark_i94 : [\"i94_cicid\",\"i94_country_residence\",\"i94_port_code\",\"i94_arrival_date\"],\n",
    "      df_spark_us_temp : [\"i94_port_code\",\"city\",\"date\",\"avg_temperature\",\"latitude\",\"longitude\"],\n",
    "      df_spark_date : [\"date\"]}\n",
    "\n",
    "for df,colNames in dq.items():\n",
    "    for colName in colNames:\n",
    "        count = 0\n",
    "        count = df.where(F.col(colName).isNull()).count()\n",
    "        print(\"The null record count:\" + str(count))\n",
    "        if (count == 0):\n",
    "            print(\"Data quality check passed for column \" + colName)\n",
    "        else:\n",
    "            print(\"Data quality check failed for column \" + colName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "##### fact_immigration: The table  will contain the immigration details for travellers travelling in the US.\n",
    "1. i94_cicid = unique id assigned to each row obtained from i94_apr16_sub.sas7bdat(cicid)\n",
    "2. i94_country_origin = coutry of origin obtained from I94CIT_I94RES_lookup.csv(value)\n",
    "3. i94_country_residence = country of residence obtained from I94CIT_I94RES_lookup.csv(value)\n",
    "4. i94_port_code = 3 character i94 port code specifying the port of entry in the USA obtained from i94_apr16_sub.sas7bdat(i94port) \n",
    "5. i94_arrival_date = Arrival date in the USA obtained from i94_apr16_sub.sas7bdat(arrdate)\n",
    "6. i94_dep_date = Departure date from the USA i94_apr16_sub.sas7bdat(depdate)\n",
    "7. i94_mode_travel = Mode of travel obtained from i94_apr16_sub.sas7bdat(i94mode)\n",
    "8. i94_visa_type = Visa type issued obtained from i94_apr16_sub.sas7bdat(i94visa)\n",
    "9. i94_gender = Gender of the traveller obtained from i94_apr16_sub.sas7bdat(gender)\n",
    "10. i94_age = Age of the traveller obtained from i94_apr16_sub.sas7bdat(i94bir and biryear)\n",
    "11. year = Year of arrival date\n",
    "\n",
    "##### dim_temperature: The table will contain the temperature for USA across various cities post 1970.\n",
    "1. i94_port_code = 3 character i94 port code from the immigration dataset obtained from I94PORT_lookup.csv(code)\n",
    "2. city = City where the temperature was recorded obtained from GlobalLandTemperaturesByCity.csv(city). \n",
    "3. state = State details for the i94 port from the immigration dataset obtained from I94PORT_lookup.csv(state)\n",
    "4. date = Date when the temperature was recorded obtained from GlobalLandTemperaturesByCity.csv(dt). \n",
    "5. avg_temperature = Average temperature in the city obtained from GlobalLandTemperaturesByCity.csv(AverageTemperature). \n",
    "6. avg_temperature_un = Average temperature uncertainity in the city obtained from GlobalLandTemperaturesByCity.csv(AverageTemperatureUncertainty). \n",
    "7. latitude = latitude where ther temperature was recorded obtained from GlobalLandTemperaturesByCity.csv(Latitude). \n",
    "8. longitude = longitude where the temperature was recorded obtained from GlobalLandTemperaturesByCity.csv(Longitude).\n",
    "9. year = year of date when the tempertature was recorded.\n",
    "\n",
    "##### dim_date : The table will contain various date dimensions for the arrival and deperature dates from the immigration dataset.\n",
    "1. date = arrival date obtained from i94_apr16_sub.sas7bdat(arrdate) \n",
    "2. year = year of arrival date\n",
    "3. month =  month of arrival date\n",
    "4. day = day of arrival date \n",
    "5. week = week of the year of arrival date\n",
    "6. weekday = day of the week of arrival date \n",
    "7. quarter = quarter of arrival date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* The i94 immigration data for april 2016 itself contains close to 3 million records which would only exponentially increase with every year. To handle such huge record volume, Spark is used taking advantage of its in memory processing capabilities which will provide huge performance boots while running the ETL pipeline. \n",
    "* The final tables are stored in parquet format with dimension modelling so that the data can be easily queried and joined to extract meaningful insights again using Spark's sql like capabilities.\n",
    "* Parquet file format is binary and compressed , hence would occupy less space. Since its a columnar storage, one can easily perform column wise queries which will be faster.\n",
    "* Update Frequency: Since the end goal is to study the impact of temperature on the travel pattern of people coming to the United states over the years, which is more of analytical nature, the data can be updated on a monthly basis.We dont need data real time.\n",
    "* Incase the data was increased by 100x, we would store the data again in parquet format on Amazon S3 which is cheap.\n",
    "* Incase the data populates a dashboard that must be updated on a daily basis by 7am every day, we would automate the data pipeline and data quality checks using Apache Airflow.\n",
    "* Incase the database needed to be accessed by 100+ people, we would host the tables on Amazon Redshift whose MPP capabilities and columnar storage would boost the query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
